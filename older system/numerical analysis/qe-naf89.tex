% latex file
\def\hcorrection#1{\advance\hoffset by #1 }
\def\vcorrection#1{\advance\voffset by #1 }

\documentclass{article}
\usepackage{my,amsxtra,amssymb,amsthm}

\vcorrection{-1.0in}
\hcorrection{-0.8in}
\textwidth 6.0in
\textheight 9.0in

\def\R{\mathbb R}
\def\C{\mathbb C}
\def\N{\mathbb N}
\def\Z{\mathbb Z}
\def\Q{\mathbb Q}

\begin{document}
%\begin{Large}






\begin{center}\begin{LARGE}
{\bf Numerical Analysis Qualifying Exam}\\ 
{\bf Fall 1989}\\ \end{LARGE}
\end{center}
\vspace{0.1in}
\noindent\hrulefill\\

\begin{description}
\item[1.]
Suppose we want to compute the value $J_{100}(1)$ by using the values
$J_0(1)$ and $J_1(1)$ (which are assumed known) and the recursion.
$$J_{m+1}(1) = 2mJ_m (1) - J_{m-1} (1).$$
Is this calculation numerically stable? Explain in detail.

\item[2.]
Let $g:[a,b] \to R$ be a $C^{(1)}$ function and suppose $p \in (a,b)$
satisfies the conditions:
$$p=g(p), \quad |g^\prime (p)| < 1.$$
Prove that there exists a $\delta > 0$ such that if $|x_0 - p| < \delta$,
then the sequence $\{x_n\}_n$ generated by the iteration
$$x_{n+1} = g(x_n)$$
converges to $p$.

\item[3.]
Given a set of data points $\{(x_i, y_i), i =1, \dots, m\}$, it is desired to
fit the data with a polynomial of degree $n < m-1$ using the method of least
squares. Thus, if we let
$$p(x) = \sum^n_{k=0} a_kx^k,$$
then we must choose the coefficients $a_0, a_1, \dots, a_n$ so as to
minimize the expression
$$E= \sum^m_{i=1} (y_i - p(x_i))^2.$$

\item[\quad] (a)
Show that the coefficients $a_0, a_1, \dots, a_n$ must satisfy the matrix
equation (called the normal equation):
$$\left[\begin{array}{cccc}
        \sum x^0_i & \sum x^1_i & \dots & \sum x^n_i \\
        \sum x^1_i & \sum x^2_i & \dots & \sum x^{n+1}_i \\
        \vdots & \vdots & & \vdots \\
        \sum x^n_i & \sum x^{n+1}_i & \dots & \sum x^{2n}_i
        \end{array}
        \right]
  \left[\begin{array} {c}
        a_0 \\
        a_1 \\
        \vdots \\
        a_n
        \end{array} \right] =
   \left[\begin{array} {c}
        \sum y_i x^0_i \\
        \sum y_i x^1_i \\
        \vdots \\
        \sum y_i x^n_i
        \end{array} \right]_,$$
where
$$\sum \equiv \sum^m_{i=1}.$$

\item[\quad] (b)
Show that the coefficient matrix of the above system is nonsingular and hence
the system has a unique solution. ({\bf Hint:} If we denote the coefficient
matrix by ${\bf C}$, then if ${\bf C}$ were singular there would be a vector
${\bf b}$ with ${\bf Cb = 0}$. Then show that the polynomial
$$q_n(x) = b_0 + b_1x + \dots + b_n x^n$$
has more than $n$ roots.)

\item[4.]
An iteration method
$$x_{n+1} = g(x_n), \quad n \geq 0, \eqno{(1)}$$
is to be used to find a fixed point of $g(x)$, such that $x=g(x)$.

\item[\quad] (a)
If $\alpha$ is a fixed point of $g(x), g(x), g^\prime(x), \dots, g^{(p)} (x)$
are continuous for all $x$ near $\alpha$ for some $p \geq 2$. Furthermore,
assume
$$g^\prime (\alpha) = \dots = g^{(p-1)} (\alpha) = 0,$$
and the initial quess $x_0$ is sufficiently close to $\alpha$. Show that the
iteration (1) has order of convergence $p$, i.e.
$$|x_{n+1} - \alpha| \leq c |x_n - \alpha|^p, \quad c: \hbox{\ constant}.$$

\item[\quad] (b)
A modification of Newton's method for solving the equation $f(x) = 0$
is Steffenson's method, defined as follows:

Choose a starting point $x_0$ and then iterate as follows:
$$x_{n+1} = x_n - \frac{f(x_n)}{D(x_n)},$$
where
$$D(x) = \frac{f(x+ f(x)) - f(x)}{f(x)}.$$
Assuming the sequence $\{x_n\}_n$ converges to the root $\alpha$ and assuming
$f^\prime (\alpha) \neq 0$, show that the convergence is second order.
(Hint: Write the iteration as $x=g(x)$. Use
$f(x) = (x-\alpha) h(x)$ with $h(\alpha) \neq 0$, then compute
the formula for $g(x)$ in terms of $h(x))$.

\item[5.]
If $A$ is a Hermitian matrix show
$$\parallel A \parallel_2 = \rho (A),$$
where $\parallel A \parallel_2$ is the Euclidean norm of $A$ and $\rho(A)$
is the spectral radius of $A$. (Hint: use an appropriate theorem and then
show a upper triangular Hermitian matrix is diagonal).

\item[6.]
Consider the discrete analog of the eigenvalue problem
$$y^{\prime \prime} + \lambda y =0, \quad 0 < x < \pi, $$
$$y(0) = y(\pi) = 0, $$
given by
$$\frac{y_{i+1} + y_{i-1} 2y_i}{(\Delta x)^2} + \lambda y_i = 0,$$
$$y_0 = y_N = 0,$$
defined on the uniform mesh $0= x_0 < x_1 < \dots < x_N = \pi$. Compute
the eigenvalues of the discrete problem by solving the finite difference
equation. How do these eigenvalues compare with those of the continuous
problems?

\item[7.]
Suppose we want to solve the quadratic equation $x^2 + bx + c = 0$ for $x$,
where $b,c$ are from measurement and $b^2 - 4ac >0$. Suppose
$b^\ast, c^\ast$ are given to us as approximate values of $b,c$. The
absolute error of them is $\hbox{Err\ } (b^\ast) \leq \varepsilon_1$,
$\hbox{Err\ } (c^\ast) \leq \varepsilon_2$ for some small
$\varepsilon_1, \varepsilon_2$. Round-off error is negligible. Give an
estimate of the absolute error and relative error of the solution $x$
in terms of known quantities.

\item[8.]
Let $A$ and $B$ have order $n$, with $A$ nonsingular. Consider solving the
linear system
$$Az_1 + Bz_2 = b_1, \quad Bz_1 + Az_2 = b_2$$
with $z_1, z_2, b_1, b_2 \in {\bf R}^n$.

\item[\quad] (a)
Find necessary and sufficient conditions for convergence of the iteration
method
$$Az_1^{(m+1)} = b_1 - Bz_2^{(m)} \quad Az_2^{(m+1)} = b_2 - Bz_1^{(m)}
  \quad m \geq 0.$$

\item[\quad] (b)
Repeat part (a) for the iteration method
$$Az_1^{(m+1)} = b_1 - Bz_2^{(m)} \quad Az_2^{(m+1)} = b_2 - Bz_1^{(m+1)}
  \quad m \geq 0.$$

Compare the convergence rates of the two methods.

\item[9.]
If 8-point Gauss-Legendre quadrature is used to compute
$$\int^1_{-1} \frac{x^2}{\sqrt{1-x^2}} dx.$$

The computed answer is 1.36468684, while the correct answer to nine figures
1.57079633.

\item[\quad] (a)
What is likely to be the cause of such a large error?

\item[\quad] (b)
What can be done to allow use of the 8-point Gaussian rule to compute
the answer accurately?

\item[10.]
Let $A$ be a Hermitian matrix, $x$ a non-zero vector. For a given
$\lambda \in {\bf C}$ let $\eta (\lambda) = Ax- \lambda x$.
Show that $\parallel \eta (\lambda) \parallel$ is minimized by taking
$$\lambda = \frac{x^\ast Ax}{x^\ast x}.$$







\end{description}    
%\end{Large}
\end{document}














